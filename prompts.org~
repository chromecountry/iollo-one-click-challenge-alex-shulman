* priming

GLOBAL CONSTRAINTS
- This is MOCK/DRY-RUN ONLY. Do NOT run `terraform apply`, provision cloud resources, or require a real AWS account. We only need: `terraform fmt` + `terraform validate` + a mock `plan`.
- Keep secrets out of code. Assume env vars and `.env.example`.
- Keep outputs small and minimal; prioritize correctness and validation.
- Prefer Bash + Python. Minimal dependencies.
- Repo name: `iollo-one-click-challenge-<MY-NAME>` (use a placeholder until I confirm my name).

PROCESS
We will work in steps. For each step:
1) Output a concise FILE TREE (what you will create/modify).
2) Provide FILE CONTENTS for each file in separate fenced code blocks labeled with their POSIX path.
3) Provide a SINGLE shell script snippet I can copy-paste to create/update all files locally (mkdir + tee/heredoc).
4) Provide a QUICK VERIFY checklist (commands I run to verify).
5) Pose any BLOCKING QUESTIONS (if none, say “No blocking questions”).
6) STOP and wait for me to say “continue to step N” before proceeding.

STEP PLAN (we will follow this order)
- Step 1: Scaffold repository structure + placeholder files (no heavy code yet).
- Step 2: Terraform module skeletons (networking, compute, dns) + `infra/envs/dev` wiring to pass `terraform validate`.
- Step 3: GitHub Actions workflow (`validate`, `plan`, `deploy` [dry], `test`) and stub scripts.
- Step 4: One-click trigger (CLI preferred) that calls `gh workflow run` or `curl` for `workflow_dispatch`.
- Step 5: DNS/SSL stub + On-prem kind/k3d stub + short on-prem README.
- Step 6: Analysis pipeline (processor, statistician, visualizer, report writer, coordinator) with mockable LLM usage.
- Step 7: README polish (“Quickstart”, “Coding Agent Usage”, assumptions/limitations).

ASSUMPTIONS (apply unless I override)
- Terraform targets AWS-style resources (e.g., VPC/subnets/SG) but we only validate/plan; no apply.
- `size` variable (small/medium/large) maps to instance counts/types as simple locals.
- `GITHUB_TOKEN` and `ANTHROPIC_API_KEY` are read from env; provide `.env.example`.
- All DNS/SSL and deploy steps are echo stubs or commented AWS CLI samples, idempotent-ish.

NOW DO ONLY STEP 1.

STEP 1: SCAFFOLD
Create the repo scaffold and placeholders:
- `README.md` with section placeholders (Quickstart + Coding Agent Usage).
- `.github/workflows/deploy.yml` (empty jobs, TODO markers).
- `infra/modules/{networking,compute,dns}/` each with empty `main.tf` + `variables.tf` placeholders.
- `infra/envs/dev/main.tf` placeholder that will later wire modules.
- `scripts/{deploy_infrastructure.sh,run_smoke_checks.sh,configure_dns.sh}` with echo stubs and `set -euo pipefail`.
- `cli/deploy-customer.sh` stub with arg parsing + usage text (no real logic yet).
- `onprem/bootstrap.sh` stub + `onprem/README.md`.
- `analysis/{processor.py,statistician.py,visualizer.py,report_writer.py,coordinator.py}` with minimal `if __name__ == "__main__": print("stub")`.
- `automation/Makefile` with `demo` and `analyze` targets (echo only for now).
- `.env.example` with `GITHUB_TOKEN=` and `ANTHROPIC_API_KEY=` placeholders.
- `.gitignore` for Python, Terraform, Node (if any), venvs, `*.tfstate`, `__pycache__`, `.env`.
- `LICENSE` (MIT) placeholder.

Remember the PROCESS output format and STOP after Step 1.

* terraform skeletons

continue to step 2

STEP 2: TERRAFORM MODULE SKELETONS

TASK
Fill in minimal Terraform skeletons for infra/modules/{networking,compute,dns} and wire them together in infra/envs/dev.

DETAILS
- Each module must have a main.tf + variables.tf + outputs.tf (create outputs.tf if missing).
- All resources should be AWS-style (VPC, subnets, instance, route53 zone/record), BUT remain safe for dry-run. No terraform apply, only fmt/validate/plan.
- Use variables: customer_id, region, size (small/medium/large). Size maps to instance_type and instance_count via a locals block in compute.
- Add comments in code marking them as mock/dry-run resources.
- Add a provider block in infra/envs/dev/main.tf that pins AWS provider to a dummy region (e.g., us-east-1).
- Wire modules together in infra/envs/dev/main.tf:
  - networking → compute (subnet IDs output)
  - dns → networking (VPC ID output if needed)
- Include terraform.tfvars.example in infra/envs/dev/ with customer_id, region, size values.

PROCESS
As before, provide:
1. File tree of new/updated files.
2. File contents for each (with fenced code blocks labeled by file path).
3. Shell script snippet (mkdir + tee) to update all files locally.
4. Quick verify checklist (commands to run).
5. Blocking questions if any.
STOP after this step, do not continue further.
